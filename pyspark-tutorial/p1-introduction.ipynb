{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting to know PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export SPARK_LOCAL_IP=192.168.193.254"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/02 21:53:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x1089ca250>\n"
     ]
    }
   ],
   "source": [
    "# Import SparkSession from pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Print spark\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing tables\n",
    "\n",
    "SparkSessioncó một thuộc tính được gọi cataloglà liệt kê tất cả dữ liệu bên trong cụm. Thuộc tính này có một vài phương pháp để trích xuất các mẩu thông tin khác nhau.\n",
    "\n",
    "Một trong những phương thức hữu ích nhất là .listTables()phương thức trả về tên của tất cả các bảng trong cụm của bạn dưới dạng danh sách."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Print the tables in the catalog\n",
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query\n",
    "\n",
    "- Use the `.sql()` method to get the first 10 rows of the flights table and save the result to flights10. The variable query contains the appropriate SQL query.\n",
    "- Use the DataFrame method `.show()` to print flights10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change this query\n",
    "query = \"FROM flights SELECT * LIMIT 10\"\n",
    "\n",
    "# Get the first 10 rows of flights\n",
    "flights10 = my_spark.sql(query)\n",
    "\n",
    "# Show the results\n",
    "flights10.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandafy a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change this query\n",
    "query = \"SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest\"\n",
    "\n",
    "# Run the query\n",
    "flight_counts = spark.sql(query)\n",
    "\n",
    "# Convert the results to a pandas DataFrame\n",
    "pd_counts = flight_counts.toPandas()\n",
    "\n",
    "# Print the head of pd_counts\n",
    "print(pd_counts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put some Spark in your data\n",
    "\n",
    "![](images/spark_figure.png)\n",
    "\n",
    "- The `.createDataFrame()` method takes a pandas DataFrame and returns a Spark DataFrame.\n",
    "- The output of this method is stored locally, not in the SparkSession catalog\n",
    "- You can do this using the `.createTempView()` Spark DataFrame method, which takes as its only argument the name of the temporary table you'd like to register. This method registers the DataFrame as a table in the catalog, but as this table is temporary, it can only be accessed from the specific SparkSession used to create the Spark DataFrame.\n",
    "- There is also the method .createOrReplaceTempView(). This safely creates a new temporary table if nothing was there before, or updates an existing table if one was already defined. You'll use this method to avoid running into problems with duplicate tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='temp', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create pd_temp\n",
    "pd_temp = pd.DataFrame(np.random.random(10))\n",
    "# Create spark_temp from pd_temp\n",
    "spark_temp = spark.createDataFrame(pd_temp)\n",
    "# Examine the tables in the catalog\n",
    "print(spark.catalog.listTables())\n",
    "\n",
    "# Add spark_temp to the catalog\n",
    "spark_temp.createOrReplaceTempView(\"temp\")\n",
    "# Examine the tables in the catalog again\n",
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping the middle man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>latitude_deg</th>\n",
       "      <th>longitude_deg</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>scheduled_service</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>home_link</th>\n",
       "      <th>wikipedia_link</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6523</td>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>40.070801</td>\n",
       "      <td>-74.933601</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>no</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>323361</td>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>38.704022</td>\n",
       "      <td>-101.473911</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>no</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6524</td>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>59.949200</td>\n",
       "      <td>-151.695999</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>no</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6525</td>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>34.864799</td>\n",
       "      <td>-86.770302</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>no</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6526</td>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>35.608700</td>\n",
       "      <td>-91.254898</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id ident           type                                name  \\\n",
       "0    6523   00A       heliport                   Total Rf Heliport   \n",
       "1  323361  00AA  small_airport                Aero B Ranch Airport   \n",
       "2    6524  00AK  small_airport                        Lowell Field   \n",
       "3    6525  00AL  small_airport                        Epps Airpark   \n",
       "4    6526  00AR         closed  Newport Hospital & Clinic Heliport   \n",
       "\n",
       "   latitude_deg  longitude_deg  elevation_ft continent iso_country iso_region  \\\n",
       "0     40.070801     -74.933601          11.0       NaN          US      US-PA   \n",
       "1     38.704022    -101.473911        3435.0       NaN          US      US-KS   \n",
       "2     59.949200    -151.695999         450.0       NaN          US      US-AK   \n",
       "3     34.864799     -86.770302         820.0       NaN          US      US-AL   \n",
       "4     35.608700     -91.254898         237.0       NaN          US      US-AR   \n",
       "\n",
       "   municipality scheduled_service gps_code iata_code local_code home_link  \\\n",
       "0      Bensalem                no      00A       NaN        00A       NaN   \n",
       "1         Leoti                no     00AA       NaN       00AA       NaN   \n",
       "2  Anchor Point                no     00AK       NaN       00AK       NaN   \n",
       "3       Harvest                no     00AL       NaN       00AL       NaN   \n",
       "4       Newport                no      NaN       NaN        NaN       NaN   \n",
       "\n",
       "  wikipedia_link keywords  \n",
       "0            NaN      NaN  \n",
       "1            NaN      NaN  \n",
       "2            NaN      NaN  \n",
       "3            NaN      NaN  \n",
       "4            NaN     00AR  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df = pd.read_csv('sample_data/airports.csv')\n",
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------------+--------------------+------------------+-------------------+------------+---------+-----------+----------+------------+-----------------+--------+---------+----------+---------+--------------+--------+\n",
      "|    id|ident|         type|                name|      latitude_deg|      longitude_deg|elevation_ft|continent|iso_country|iso_region|municipality|scheduled_service|gps_code|iata_code|local_code|home_link|wikipedia_link|keywords|\n",
      "+------+-----+-------------+--------------------+------------------+-------------------+------------+---------+-----------+----------+------------+-----------------+--------+---------+----------+---------+--------------+--------+\n",
      "|  6523|  00A|     heliport|   Total Rf Heliport|    40.07080078125| -74.93360137939453|          11|       NA|         US|     US-PA|    Bensalem|               no|     00A|     null|       00A|     null|          null|    null|\n",
      "|323361| 00AA|small_airport|Aero B Ranch Airport|         38.704022|        -101.473911|        3435|       NA|         US|     US-KS|       Leoti|               no|    00AA|     null|      00AA|     null|          null|    null|\n",
      "|  6524| 00AK|small_airport|        Lowell Field|       59.94919968|     -151.695999146|         450|       NA|         US|     US-AK|Anchor Point|               no|    00AK|     null|      00AK|     null|          null|    null|\n",
      "|  6525| 00AL|small_airport|        Epps Airpark| 34.86479949951172| -86.77030181884766|         820|       NA|         US|     US-AL|     Harvest|               no|    00AL|     null|      00AL|     null|          null|    null|\n",
      "|  6526| 00AR|       closed|Newport Hospital ...|           35.6087|         -91.254898|         237|       NA|         US|     US-AR|     Newport|               no|    null|     null|      null|     null|          null|    00AR|\n",
      "|322127| 00AS|small_airport|      Fulton Airport|        34.9428028|        -97.8180194|        1100|       NA|         US|     US-OK|        Alex|               no|    00AS|     null|      00AS|     null|          null|    null|\n",
      "|  6527| 00AZ|small_airport|      Cordes Airport|34.305599212646484|-112.16500091552734|        3810|       NA|         US|     US-AZ|      Cordes|               no|    00AZ|     null|      00AZ|     null|          null|    null|\n",
      "|  6528| 00CA|small_airport|Goldstone /Gts/ A...|35.350498199499995|     -116.888000488|        3038|       NA|         US|     US-CA|     Barstow|               no|    00CA|     null|      00CA|     null|          null|    null|\n",
      "|324424| 00CL|small_airport| Williams Ag Airport|         39.427188|        -121.763427|          87|       NA|         US|     US-CA|       Biggs|               no|    00CL|     null|      00CL|     null|          null|    null|\n",
      "|322658| 00CN|     heliport|Kitchen Creek Hel...|        32.7273736|       -116.4597417|        3350|       NA|         US|     US-CA| Pine Valley|               no|    00CN|     null|      00CN|     null|          null|    null|\n",
      "|  6529| 00CO|       closed|          Cass Field|         40.622202|        -104.344002|        4830|       NA|         US|     US-CO|  Briggsdale|               no|    null|     null|      null|     null|          null|    00CO|\n",
      "|  6531| 00FA|small_airport| Grass Patch Airport| 28.64550018310547| -82.21900177001953|          53|       NA|         US|     US-FL|    Bushnell|               no|    00FA|     null|      00FA|     null|          null|    null|\n",
      "|  6532| 00FD|     heliport|  Ringhaver Heliport|28.846599578857422| -82.34539794921875|          25|       NA|         US|     US-FL|   Riverview|               no|    00FD|     null|      00FD|     null|          null|    null|\n",
      "|  6533| 00FL|small_airport|   River Oak Airport|27.230899810791016| -80.96920013427734|          35|       NA|         US|     US-FL|  Okeechobee|               no|    00FL|     null|      00FL|     null|          null|    null|\n",
      "|  6534| 00GA|small_airport|    Lt World Airport| 33.76750183105469| -84.06829833984375|         700|       NA|         US|     US-GA|    Lithonia|               no|    00GA|     null|      00GA|     null|          null|    null|\n",
      "|  6535| 00GE|     heliport|    Caffrey Heliport|         33.889245|          -84.73793|         957|       NA|         US|     US-GA|       Hiram|               no|    00GE|     null|      00GE|     null|          null|    null|\n",
      "|  6536| 00HI|     heliport|  Kaupulehu Heliport|         19.832715|        -155.980233|          43|       NA|         US|     US-HI| Kailua-Kona|               no|    00HI|     null|      00HI|     null|          null|    null|\n",
      "|  6537| 00ID|small_airport|Delta Shores Airport|48.145301818847656|-116.21399688720703|        2064|       NA|         US|     US-ID|  Clark Fork|               no|    00ID|     null|      00ID|     null|          null|    null|\n",
      "|322581| 00IG|small_airport|       Goltl Airport|         39.724028|        -101.395994|        3359|       NA|         US|     US-KS|    McDonald|               no|    00IG|     null|      00IG|     null|          null|    null|\n",
      "|  6538| 00II|       closed|Bailey Generation...|         41.644501|         -87.122803|         600|       NA|         US|     US-IN|  Chesterton|               no|    null|     null|      null|     null|          null|    00II|\n",
      "+------+-----+-------------+--------------------+------------------+-------------------+------------+---------+-----------+----------+------------+-----------------+--------+---------+----------+---------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = \"sample_data/airports.csv\"\n",
    "\n",
    "# Read in the airports data\n",
    "airports = spark.read.csv(file_path, header=True)\n",
    "\n",
    "# Show the data\n",
    "airports.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulatng data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame flights\n",
    "flights = spark.table(\"flights\")\n",
    "\n",
    "# Show the head\n",
    "flights.show()\n",
    "\n",
    "# Add duration_hrs\n",
    "flights = flights.withColumn(\"duration_hrs\", flights.air_time/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter flights by passing a string\n",
    "long_flights1 = flights.filter(\"distance > 1000\")\n",
    "\n",
    "# Filter flights by passing a column of boolean values\n",
    "long_flights2 = flights.filter(flights.distance > 1000)\n",
    "\n",
    "long_flights1.show(5)\n",
    "long_flights2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first set of columns\n",
    "selected1 = flights.select(\"tailnum\", \"origin\", \"dest\")\n",
    "\n",
    "# Select the second set of columns\n",
    "temp = flights.select(flights.origin, flights.dest, flights.carrier)\n",
    "\n",
    "# Define first filter\n",
    "filterA = flights.origin == \"SEA\"\n",
    "\n",
    "# Define second filter\n",
    "filterB = flights.dest == \"PDX\"\n",
    "\n",
    "# Filter the data, first by filterA then by filterB\n",
    "selected2 = temp.filter(filterA).filter(filterB)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can also use the `.alias()` method to rename a column you're selecting\n",
    "- The equivalent Spark DataFrame method `.selectExpr()` takes SQL expressions as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define avg_speed\n",
    "avg_speed = (flights.distance / (flights.air_time/60)).alias(\"avg_speed\")\n",
    "\n",
    "# Select the correct columns\n",
    "speed1 = flights.select(\"origin\", \"dest\", \"tailnum\", avg_speed)\n",
    "\n",
    "# Create the same table using a SQL expression\n",
    "speed2 = flights.selectExpr(\"origin\", \"dest\", \"tailnum\", \"distance/(air_time/60) as avg_speed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating - Tổng hợp.\n",
    "\n",
    "- `min()`\n",
    "- `max()`\n",
    "- `count()`\n",
    "- `groupBy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the shortest flight from PDX in terms of distance\n",
    "flights.filter(flights.origin == 'PDX').groupBy().min('distance').show()\n",
    "\n",
    "# Find the longest flight from SEA in terms of air time\n",
    "flights.filter(flights.origin == 'SEA').groupBy().max('air_time').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average duration of Delta flights\n",
    "\n",
    "# Use the .avg() method to get the average air time of Delta Airlines flights (where the carrier column has the value \"DL\") that left SEA. \n",
    "# The place of departure is stored in the column origin. show() the result\n",
    "\n",
    "flights.filter(flights.carrier == \"DL\").filter(flights.origin == \"SEA\").groupBy().avg(\"air_time\").show()\n",
    "\n",
    "# Use the .sum() method to get the total number of hours all planes in this dataset spent in the air \n",
    "# by creating a column called duration_hrs from the column air_time. show() the result.\n",
    "\n",
    "flights.withColumn(\"duration_hrs\", flights.air_time/60).groupBy().sum(\"duration_hrs\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping and Aggregating I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by tailnum\n",
    "by_plane = flights.groupBy('tailnum')\n",
    "\n",
    "# Number of flights each plane made\n",
    "by_plane.count().show()\n",
    "\n",
    "# Group by origin\n",
    "by_origin = flights.groupBy('origin')\n",
    "\n",
    "# Average duration of flights from PDX and SEA\n",
    "by_origin.avg('air_time').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the `GroupedData` methods you've already seen, there is also the `.agg()` method. \n",
    "\n",
    "This method lets you pass an aggregate column expression that uses any of the aggregate functions from the `pyspark.sql.functions` submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Group by month and dest\n",
    "by_month_dest = flights.groupBy('month', 'dest')\n",
    "\n",
    "# Average departure delay by month and destination\n",
    "by_month_dest.avg('dep_delay').show()\n",
    "\n",
    "# Standard deviation of departure delay\n",
    "by_month_dest.agg(F.stddev('dep_delay')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining\n",
    "\n",
    "In PySpark, joins are performed using the DataFrame method `.join()`. This method takes three arguments. \n",
    "- The first is the second DataFrame that you want to join with the first one. \n",
    "- The second argument, `on`, is the name of the key column(s) as a string. The names of the key column(s) must be the same in each table. \n",
    "- The third argument, how, specifies the kind of join to perform. In this course we'll always use the value` how=\"leftouter\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the airports DataFrame by calling .show(). \n",
    "# Note which key column will let you join airports to the flights table.\n",
    "print(airports.show())\n",
    "\n",
    "# Rename the faa column in airports to dest by re-assigning \n",
    "# the result of airports.withColumnRenamed(\"faa\", \"dest\") to airports.\n",
    "airports = airports.withColumnRenamed(\"faa\", \"dest\")\n",
    "\n",
    "\n",
    "# Join the flights with the airports DataFrame on the dest column by calling the .join()\n",
    "# method on flights. Save the result as flights_with_airports.\n",
    "flights_with_airports =flights.join(airports, on=\"dest\", how=\"leftouter\")\n",
    "\n",
    "# Examine the new DataFrame\n",
    "print(flights_with_airports.show())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f84cfd82a30>\n"
     ]
    }
   ],
   "source": [
    "# spark.stop()\n",
    "# Import SparkSession from pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "# Create spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Print spark\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------------------+----------------+--------+-------+-----+-----+---------+\n",
      "|tailnum|year|                type|    manufacturer|   model|engines|seats|speed|   engine|\n",
      "+-------+----+--------------------+----------------+--------+-------+-----+-----+---------+\n",
      "| N102UW|1998|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N103US|1999|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N104UW|1999|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N105UW|1999|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N107US|1999|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N108UW|1999|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N109UW|1999|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N110UW|1999|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N111US|1999|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N11206|2000|Fixed wing multi ...|          BOEING| 737-824|      2|  149|   NA|Turbo-fan|\n",
      "| N112US|1999|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N113UW|1999|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N114UW|1999|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N117UW|2000|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N118US|2000|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N119US|2000|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N1200K|1998|Fixed wing multi ...|          BOEING| 767-332|      2|  330|   NA|Turbo-fan|\n",
      "| N1201P|1998|Fixed wing multi ...|          BOEING| 767-332|      2|  330|   NA|Turbo-fan|\n",
      "| N12114|1995|Fixed wing multi ...|          BOEING| 757-224|      2|  178|   NA|Turbo-jet|\n",
      "| N121DE|1987|Fixed wing multi ...|          BOEING| 767-332|      2|  330|   NA|Turbo-fan|\n",
      "+-------+----+--------------------+----------------+--------+-------+-----+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "planes = spark.read.csv('datasets/planes.csv', header=True)\n",
    "planes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|\n",
      "|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA| SFO|     111|     679|  14|    43|\n",
      "|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX| SJC|      83|     569|  17|     5|\n",
      "|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA| BUR|     127|     937|   7|    54|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights = spark.read.csv('datasets/flights_small.csv',header=True)\n",
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join the DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, rename the year column of planes to plane_year \n",
    "# to avoid duplicate column names.\n",
    "planes = planes.withColumnRenamed(\"year\", \"plane_year\")\n",
    "\n",
    "# Create a new DataFrame called `model_data` by joining the `flights` table with `planes` \n",
    "# using the `tailnum` column as the key\n",
    "model_data = flights.join(planes, on=\"tailnum\", how=\"leftouter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+---+--------+---------+--------+---------+-------+------+------+----+--------+--------+----+------+----------+--------------------+------------+--------+-------+-----+-----+---------+\n",
      "|tailnum|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|flight|origin|dest|air_time|distance|hour|minute|plane_year|                type|manufacturer|   model|engines|seats|speed|   engine|\n",
      "+-------+----+-----+---+--------+---------+--------+---------+-------+------+------+----+--------+--------+----+------+----------+--------------------+------------+--------+-------+-----+-----+---------+\n",
      "| N846VA|2014|   12|  8|     658|       -7|     935|       -5|     VX|  1780|   SEA| LAX|     132|     954|   6|    58|      2011|Fixed wing multi ...|      AIRBUS|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N559AS|2014|    1| 22|    1040|        5|    1505|        5|     AS|   851|   SEA| HNL|     360|    2677|  10|    40|      2006|Fixed wing multi ...|      BOEING| 737-890|      2|  149|   NA|Turbo-fan|\n",
      "| N847VA|2014|    3|  9|    1443|       -2|    1652|        2|     VX|   755|   SEA| SFO|     111|     679|  14|    43|      2011|Fixed wing multi ...|      AIRBUS|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N360SW|2014|    4|  9|    1705|       45|    1839|       34|     WN|   344|   PDX| SJC|      83|     569|  17|     5|      1992|Fixed wing multi ...|      BOEING| 737-3H4|      2|  149|   NA|Turbo-fan|\n",
      "| N612AS|2014|    3|  9|     754|       -1|    1015|        1|     AS|   522|   SEA| BUR|     127|     937|   7|    54|      1999|Fixed wing multi ...|      BOEING| 737-790|      2|  151|   NA|Turbo-jet|\n",
      "+-------+----+-----+---+--------+---------+--------+---------+-------+------+------+----+--------+--------+----+------+----------+--------------------+------------+--------+-------+-----+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String to integer\n",
    "\n",
    "`dataframe = dataframe.withColumn(\"col\", dataframe.col.cast(\"new_type\"))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+---+--------+---------+--------+---------+-------+------+------+----+--------+--------+----+------+----------+--------------------+------------+--------+-------+-----+-----+---------+\n",
      "|tailnum|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|flight|origin|dest|air_time|distance|hour|minute|plane_year|                type|manufacturer|   model|engines|seats|speed|   engine|\n",
      "+-------+----+-----+---+--------+---------+--------+---------+-------+------+------+----+--------+--------+----+------+----------+--------------------+------------+--------+-------+-----+-----+---------+\n",
      "| N846VA|2014|   12|  8|     658|       -7|     935|       -5|     VX|  1780|   SEA| LAX|     132|     954|   6|    58|      2011|Fixed wing multi ...|      AIRBUS|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N559AS|2014|    1| 22|    1040|        5|    1505|        5|     AS|   851|   SEA| HNL|     360|    2677|  10|    40|      2006|Fixed wing multi ...|      BOEING| 737-890|      2|  149|   NA|Turbo-fan|\n",
      "| N847VA|2014|    3|  9|    1443|       -2|    1652|        2|     VX|   755|   SEA| SFO|     111|     679|  14|    43|      2011|Fixed wing multi ...|      AIRBUS|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N360SW|2014|    4|  9|    1705|       45|    1839|       34|     WN|   344|   PDX| SJC|      83|     569|  17|     5|      1992|Fixed wing multi ...|      BOEING| 737-3H4|      2|  149|   NA|Turbo-fan|\n",
      "| N612AS|2014|    3|  9|     754|       -1|    1015|        1|     AS|   522|   SEA| BUR|     127|     937|   7|    54|      1999|Fixed wing multi ...|      BOEING| 737-790|      2|  151|   NA|Turbo-jet|\n",
      "+-------+----+-----+---+--------+---------+--------+---------+-------+------+------+----+--------+--------+----+------+----------+--------------------+------------+--------+-------+-----+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cast the columns to integers\n",
    "model_data = model_data.withColumn(\"arr_delay\", model_data.arr_delay.cast(\"integer\"))\n",
    "model_data = model_data.withColumn(\"air_time\", model_data.air_time.cast(\"integer\"))\n",
    "model_data = model_data.withColumn(\"month\", model_data.month.cast(\"integer\"))\n",
    "model_data = model_data.withColumn(\"plane_year\", model_data.plane_year.cast(\"integer\"))\n",
    "\n",
    "model_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the column plane_age\n",
    "model_data = model_data.withColumn(\"plane_age\", model_data.year - model_data.plane_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a Boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/03 08:47:20 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+---+--------+---------+--------+---------+-------+------+------+----+--------+--------+----+------+----------+--------------------+------------+--------+-------+-----+-----+---------+---------+-------+-----+\n",
      "|tailnum|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|flight|origin|dest|air_time|distance|hour|minute|plane_year|                type|manufacturer|   model|engines|seats|speed|   engine|plane_age|is_late|label|\n",
      "+-------+----+-----+---+--------+---------+--------+---------+-------+------+------+----+--------+--------+----+------+----------+--------------------+------------+--------+-------+-----+-----+---------+---------+-------+-----+\n",
      "| N846VA|2014|   12|  8|     658|       -7|     935|       -5|     VX|  1780|   SEA| LAX|     132|     954|   6|    58|      2011|Fixed wing multi ...|      AIRBUS|A320-214|      2|  182|   NA|Turbo-fan|      3.0|  false|    0|\n",
      "| N559AS|2014|    1| 22|    1040|        5|    1505|        5|     AS|   851|   SEA| HNL|     360|    2677|  10|    40|      2006|Fixed wing multi ...|      BOEING| 737-890|      2|  149|   NA|Turbo-fan|      8.0|   true|    1|\n",
      "| N847VA|2014|    3|  9|    1443|       -2|    1652|        2|     VX|   755|   SEA| SFO|     111|     679|  14|    43|      2011|Fixed wing multi ...|      AIRBUS|A320-214|      2|  182|   NA|Turbo-fan|      3.0|   true|    1|\n",
      "| N360SW|2014|    4|  9|    1705|       45|    1839|       34|     WN|   344|   PDX| SJC|      83|     569|  17|     5|      1992|Fixed wing multi ...|      BOEING| 737-3H4|      2|  149|   NA|Turbo-fan|     22.0|   true|    1|\n",
      "| N612AS|2014|    3|  9|     754|       -1|    1015|        1|     AS|   522|   SEA| BUR|     127|     937|   7|    54|      1999|Fixed wing multi ...|      BOEING| 737-790|      2|  151|   NA|Turbo-jet|     15.0|   true|    1|\n",
      "+-------+----+-----+---+--------+---------+--------+---------+-------+------+------+----+--------+--------+----+------+----------+--------------------+------------+--------+-------+-----+-----+---------+---------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create is_late\n",
    "model_data = model_data.withColumn(\"is_late\", model_data.arr_delay > 0)\n",
    "\n",
    "# Convert to an integer\n",
    "model_data = model_data.withColumn(\"label\", model_data.is_late.cast(\"integer\"))\n",
    "\n",
    "# Remove missing values\n",
    "model_data = model_data.filter(\"arr_delay is not NULL and dep_delay is not NULL and air_time is not NULL and distance is not NULL\")\n",
    "\n",
    "model_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneHotEncoder\n",
    "\n",
    "- Create a `StringIndexer` called `carr_indexer` by calling `StringIndexer()` with `inputCol=\"carrier\"` and `outputCol=\"carrier_index\"`.\n",
    "- Create a `OneHotEncoder` called carr_encoder by calling `OneHotEncoder()` with `inputCol=\"carrier_index\"` and `outputCol=\"carrier_fact\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# Create a StringIndexer\n",
    "carr_indexer = StringIndexer(inputCol='carrier', outputCol='carrier_index')\n",
    "\n",
    "# Create a OneHotEncoder\n",
    "carr_encoder = OneHotEncoder(inputCol='carrier_index', outputCol='carrier_fact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StringIndexer\n",
    "dest_indexer = StringIndexer(inputCol=\"dest\", outputCol=\"dest_index\")\n",
    "\n",
    "# Create a OneHotEncoder\n",
    "dest_encoder = OneHotEncoder(inputCol=\"dest_index\", outputCol=\"dest_fact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble a vector\n",
    "\n",
    "The last step in the Pipeline is to combine all of the columns containing our features into a single column. This has to be done before modeling can take place because every Spark modeling routine expects the data to be in this form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Make a VectorAssembler\n",
    "vec_assembler = VectorAssembler(inputCols=[\"month\", \"air_time\", \"carrier_fact\", \"dest_fact\", \"plane_age\"], outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Make the pipeline\n",
    "# stages: Giai doan\n",
    "flights_pipe = Pipeline(stages=[dest_indexer, dest_encoder, carr_indexer, carr_encoder, vec_assembler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Fit and transform the data\n",
    "piped_data = flights_pipe.fit(model_data).transform(model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "training, test = piped_data.randomSplit([0.6, 0.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tuning and selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create a LogisticRegression Estimator\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the evaluation submodule\n",
    "import pyspark.ml.evaluation as evals\n",
    "\n",
    "# Create a BinaryClassificationEvaluator\n",
    "evaluator = evals.BinaryClassificationEvaluator(metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tuning submodule\n",
    "import pyspark.ml.tuning as tune\n",
    "import numpy as np\n",
    "\n",
    "# Create the parameter grid\n",
    "grid = tune.ParamGridBuilder()\n",
    "\n",
    "# Add the hyperparameter\n",
    "grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\n",
    "grid = grid.addGrid(lr.elasticNetParam, [0, 1])\n",
    "\n",
    "# Build the grid\n",
    "grid = grid.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CrossValidator\n",
    "cv = tune.CrossValidator(estimator=lr,\n",
    "                        estimatorParamMaps=grid,\n",
    "                        evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_lr = lr.fit(training)\n",
    "# print(best_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = cv.fit(training)\n",
    "# best_model = models.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict the test set\n",
    "test_results = best_lr.transform(test)\n",
    "\n",
    "# Evaluate the predictions\n",
    "print(evaluator.evaluate(test_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
