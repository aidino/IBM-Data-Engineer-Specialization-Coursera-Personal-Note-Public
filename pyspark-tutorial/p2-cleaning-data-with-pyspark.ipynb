{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "people_schema = StructType([\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('age', IntegerType(), True),\n",
    "    StructField('city', StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using lazy processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/04 20:40:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('cleaning-data').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_dfw_df = spark.read.format('csv').options(Header=True).load(\"datasets/AA_DFW_2017_Departures_Short.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|       01/01/2017|         0005|                HNL|                          537|\n",
      "|       01/01/2017|         0007|                OGG|                          498|\n",
      "|       01/01/2017|         0037|                SFO|                          241|\n",
      "|       01/01/2017|         0043|                DTW|                          134|\n",
      "|       01/01/2017|         0051|                STL|                           88|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aa_dfw_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+-----------------------------+-------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|airport|\n",
      "+-----------------+-------------+-------------------+-----------------------------+-------+\n",
      "|       01/01/2017|         0005|                HNL|                          537|    hnl|\n",
      "|       01/01/2017|         0007|                OGG|                          498|    ogg|\n",
      "|       01/01/2017|         0037|                SFO|                          241|    sfo|\n",
      "|       01/01/2017|         0043|                DTW|                          134|    dtw|\n",
      "|       01/01/2017|         0051|                STL|                           88|    stl|\n",
      "+-----------------+-------------+-------------------+-----------------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "# Add the airport column using the F.lower() method\n",
    "aa_dfw_df = aa_dfw_df.withColumn('airport', F.lower(aa_dfw_df['Destination Airport']))\n",
    "aa_dfw_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Actual elapsed time (Minutes)|airport|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|       01/01/2017|         0005|                          537|    hnl|\n",
      "|       01/01/2017|         0007|                          498|    ogg|\n",
      "|       01/01/2017|         0037|                          241|    sfo|\n",
      "|       01/01/2017|         0043|                          134|    dtw|\n",
      "|       01/01/2017|         0051|                           88|    stl|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop the Destination Airport column\n",
    "aa_dfw_df = aa_dfw_df.drop(aa_dfw_df['Destination Airport'])\n",
    "aa_dfw_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a DataFrame in Parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the row count of df1 and df2\n",
    "print(\"df1 Count: %d\" % df1.count())\n",
    "print(\"df2 Count: %d\" % df2.count())\n",
    "\n",
    "# Combine the DataFrames into one\n",
    "df3 = df1.union(df2)\n",
    "\n",
    "# Save the df3 DataFrame in Parquet format\n",
    "df3.write.parquet('AA_DFW_ALL.parquet', mode='overwrite')\n",
    "\n",
    "# Read the Parquet file into a new DataFrame and run a count\n",
    "print(spark.read.parquet('AA_DFW_ALL.parquet').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL and Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Parquet file into flights_df\n",
    "flights_df = spark.read.parquet('AA_DFW_ALL.parquet')\n",
    "\n",
    "# Register the temp table\n",
    "flights_df.createOrReplaceTempView('flights')\n",
    "\n",
    "# Run a SQL query of the average flight duration\n",
    "avg_duration = spark.sql('SELECT avg(flight_duration) from flights').collect()[0]\n",
    "print('The average flight time is: %d' % avg_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating DataFrames in the real world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering column content with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|\n",
      "+----------+-------------+-------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|\n",
      "+----------+-------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "voter_df = spark.read.csv('datasets/DallasCouncilVoters.csv.gz', header=True, inferSchema=True)\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`distinct()`: Hàm `distinct()` là một hoạt động trong `PySpark`, nó loại bỏ các hàng trùng lặp từ `DataFrame` và trả về một `DataFrame` mới với các hàng duy nhất dựa trên các cột được chỉ định."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|VOTER_NAME                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Tennell Atkins                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|  the  final   2018 Assessment Plan and the 2018 Assessment  Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing  classifications  for   the   apportionment   of   costs and the methods of assessing special assessments for the services and improvements to property in the District;  closing  the  hearing  and  levying  a  special  assessment  on  property  in  the  District              |\n",
      "|Scott Griggs                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Scott  Griggs                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|Sandy Greyson                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|Michael S. Rawlings                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "| the final 2018 Assessment Plan and the 2018 Assessment  Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing  classifications  for  the   apportionment   of   costs and  the  methods  of  assessing  special  assessments  on  Dallas  hotels  with    100 or more rooms                                                                                                                           |\n",
      "|Kevin Felder                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Adam Medrano                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Casey  Thomas                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|null                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|   the   final  2018 Assessment  Plan  and  the  2018 Assessment  Roll  (to  be  kept  on  file   with the City Secretary); establishing classifications  for  the  apportionment  of  costs  and  the  methods  of  assessing  special  assessments  for  the  services  and  improvements  to  property  in  the  District;  closing  the  hearing  and  levying  a special  assessment  on  property  in  the  District |\n",
      "|011018__42                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|Mark  Clayton                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|Casey Thomas                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Sandy  Greyson                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|Mark Clayton                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Jennifer S.  Gates                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|Tiffinni A. Young                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|  the  final  2018 Assessment  Plan   and   the   2018 Assessment   Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing   classifications  for the apportionment of costs and the  methods  of  assessing  special  assessments for the services and improvements  to  property  in  the  District;  closing the hearing and  levying  a  special  assessment  on  property  in  the  District       |\n",
      "|B. Adam  McGough                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|Omar Narvaez                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Philip T. Kingston                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|Rickey D. Callahan                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|Dwaine R. Caraway                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|Philip T.  Kingston                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|Jennifer S. Gates                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|Lee M. Kleinman                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|Monica R. Alonzo                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|   the   final  2018 Assessment  Plan  and  the  2018 Assessment  Roll   (to  be  kept  on  file   with the City Secretary); establishing classifications  for  the  apportionment  of  costs  and  the  methods  of  assessing  special  assessments  for  the  services  and  improvements  to  property  in  the  District;  closing  the  hearing  and  levying  a special  assessment  on  property  in  the  District|\n",
      "|Rickey D.  Callahan                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|Carolyn King Arnold                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|  the  final   2018 Assessment Plan and the 2018 Assessment  Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing  classifications  for  the   apportionment   of   costs and the methods of assessing special assessments for the services and improvements to property in the District;  closing  the  hearing  and  levying  a  special  assessment  on  property  in  the  District               |\n",
      "|Erik Wilson                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|  the  final  2018 Assessment Plan and the 2018 Assessment  Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing  classifications  for  the   apportionment   of   costs and the methods of assessing special assessments for the services and improvements to property in the District; closing  the  hearing  and  levying  a  special  assessment  on  property  in  the  District                 |\n",
      "|Lee Kleinman                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df.select(voter_df.VOTER_NAME).distinct().show(40, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter voter_df where the VOTER_NAME is 1-20 characters in length\n",
    "voter_df = voter_df.filter('length(VOTER_NAME) > 0 and length(VOTER_NAME) < 20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out voter_df where the VOTER_NAME contains an underscore\n",
    "voter_df = voter_df.filter(~ F.col('VOTER_NAME').contains('_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|VOTER_NAME         |\n",
      "+-------------------+\n",
      "|Tennell Atkins     |\n",
      "|Scott Griggs       |\n",
      "|Scott  Griggs      |\n",
      "|Sandy Greyson      |\n",
      "|Michael S. Rawlings|\n",
      "|Kevin Felder       |\n",
      "|Adam Medrano       |\n",
      "|Casey  Thomas      |\n",
      "|Mark  Clayton      |\n",
      "|Casey Thomas       |\n",
      "|Sandy  Greyson     |\n",
      "|Mark Clayton       |\n",
      "|Jennifer S.  Gates |\n",
      "|Tiffinni A. Young  |\n",
      "|B. Adam  McGough   |\n",
      "|Omar Narvaez       |\n",
      "|Philip T. Kingston |\n",
      "|Rickey D. Callahan |\n",
      "|Dwaine R. Caraway  |\n",
      "|Philip T.  Kingston|\n",
      "|Jennifer S. Gates  |\n",
      "|Lee M. Kleinman    |\n",
      "|Monica R. Alonzo   |\n",
      "|Rickey D.  Callahan|\n",
      "|Carolyn King Arnold|\n",
      "|Erik Wilson        |\n",
      "|Lee Kleinman       |\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the distinct VOTER_NAME entries again\n",
    "voter_df.select(voter_df.VOTER_NAME).distinct().show(40, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying DataFrame columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+--------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|              splits|\n",
      "+----------+-------------+-------------------+--------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|     [Adam, Medrano]|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     [Casey, Thomas]|\n",
      "+----------+-------------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new column called splits separated on whitespace\n",
    "voter_df = voter_df.withColumn('splits', F.split(voter_df.VOTER_NAME, '\\s+'))\n",
    "voter_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+--------------------+----------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|              splits|first_name|\n",
      "+----------+-------------+-------------------+--------------------+----------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|    Philip|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|   Michael|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|     [Adam, Medrano]|      Adam|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     [Casey, Thomas]|     Casey|\n",
      "+----------+-------------+-------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new column called first_name based on the first item in splits\n",
    "voter_df = voter_df.withColumn('first_name', voter_df.splits.getItem(0))\n",
    "voter_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+--------------------+----------+---------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|              splits|first_name|last_name|\n",
      "+----------+-------------+-------------------+--------------------+----------+---------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|    Philip| Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|   Michael| Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     [Casey, Thomas]|     Casey|   Thomas|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|[Carolyn, King, A...|   Carolyn|   Arnold|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|     [Scott, Griggs]|     Scott|   Griggs|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough| [B., Adam, McGough]|        B.|  McGough|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|     [Lee, Kleinman]|       Lee| Kleinman|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|    [Sandy, Greyson]|     Sandy|  Greyson|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|    Philip| Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|   Michael| Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     [Casey, Thomas]|     Casey|   Thomas|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|[Carolyn, King, A...|   Carolyn|   Arnold|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|[Rickey, D., Call...|    Rickey| Callahan|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|    [Sandy, Greyson]|     Sandy|  Greyson|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|\n",
      "+----------+-------------+-------------------+--------------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngohongthai/miniconda3/envs/data-engineering/lib/python3.9/site-packages/pyspark/sql/column.py:458: FutureWarning: A column as 'key' in getItem is deprecated as of Spark 3.0, and will not be supported in the future release. Use `column[key]` or `column.key` syntax instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Get the last entry of the splits list and create a column called last_name\n",
    "voter_df = voter_df.withColumn(\"last_name\", voter_df.splits.getItem(F.size('splits') - 1))\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+----------+---------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|first_name|last_name|\n",
      "+----------+-------------+-------------------+----------+---------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   Carolyn|   Arnold|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|     Scott|   Griggs|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough|        B.|  McGough|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|       Lee| Kleinman|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|     Sandy|  Greyson|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   Carolyn|   Arnold|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|    Rickey| Callahan|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|     Sandy|  Greyson|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|  Jennifer|    Gates|\n",
      "+----------+-------------+-------------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop the splits column\n",
    "voter_df = voter_df.drop('splits')\n",
    "# Show the voter_df DataFrame\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### when() example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+----------+---------+-------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|first_name|last_name|         random_val|\n",
      "+----------+-------------+-------------------+----------+---------+-------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates| 0.2207577456456088|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston| 0.3945328780489654|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|               null|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano| 0.3279043771538702|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas|0.42274487832381524|\n",
      "+----------+-------------+-------------------+----------+---------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a column to voter_df named random_val with the results of the F.rand() \n",
    "# method for any voter with the title Councilmember.\n",
    "\n",
    "voter_df = voter_df.withColumn('random_val', F.when(voter_df.TITLE == 'Councilmember', F.rand()))\n",
    "voter_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When / Otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+----------+---------+-------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|first_name|last_name|         random_val|\n",
      "+----------+-------------+-------------------+----------+---------+-------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|0.26032428301487776|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston| 0.3289315117728715|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|                2.0|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano| 0.2930330374667178|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas| 0.6262461239391839|\n",
      "+----------+-------------+-------------------+----------+---------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df = voter_df.withColumn(\"random_val\", \n",
    "                               F.when(voter_df.TITLE == \"Councilmember\", F.rand()).when(voter_df.TITLE == \"Mayor\", 2).otherwise(0))\n",
    "voter_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------------+----------+---------+----------+\n",
      "|      DATE|               TITLE|       VOTER_NAME|first_name|last_name|random_val|\n",
      "+----------+--------------------+-----------------+----------+---------+----------+\n",
      "|04/25/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|04/25/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|06/20/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|06/20/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|06/20/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|06/20/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|08/15/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|08/15/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|09/18/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|09/18/2018|       Mayor Pro Tem|    Casey  Thomas|     Casey|   Thomas|       0.0|\n",
      "|04/25/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|04/25/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|04/11/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|04/11/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|04/11/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|04/11/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|04/11/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|06/13/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|06/13/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|04/11/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "+----------+--------------------+-----------------+----------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the .filter() clause with random_val\n",
    "voter_df.filter(voter_df.random_val == 0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using user defined functions in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+----------+---------+-------------------+---------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|first_name|last_name|         random_val|first_and_middle_name|\n",
      "+----------+-------------+-------------------+----------+---------+-------------------+---------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|0.26032428301487776|       Jennifer Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston| 0.3289315117728715|      Philip Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|                2.0|     Michael Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano| 0.2930330374667178|         Adam Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas| 0.6262461239391839|         Casey Thomas|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   Carolyn|   Arnold| 0.6970129546902093|       Carolyn Arnold|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|     Scott|   Griggs| 0.3030037296417559|         Scott Griggs|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough|        B.|  McGough|0.19782684966296726|           B. McGough|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|       Lee| Kleinman| 0.8913741742096785|         Lee Kleinman|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|     Sandy|  Greyson| 0.3777549953161322|        Sandy Greyson|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|0.17518490119001706|       Jennifer Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston|0.06489703602084196|      Philip Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|                2.0|     Michael Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano| 0.7540099985284854|         Adam Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas|0.18678163838439454|         Casey Thomas|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   Carolyn|   Arnold| 0.6371945874414872|       Carolyn Arnold|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|    Rickey| Callahan| 0.4912601769593956|      Rickey Callahan|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|  0.677516300861708|       Jennifer Gates|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|     Sandy|  Greyson|0.08843795639549479|        Sandy Greyson|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|  Jennifer|    Gates|0.20080545280095885|       Jennifer Gates|\n",
      "+----------+-------------+-------------------+----------+---------+-------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "def getFirstAndMiddle(names):\n",
    "  # Return a space separated string of names\n",
    "  return ' '.join(names)\n",
    "\n",
    "# Define the method as a UDF\n",
    "udfFirstAndMiddle = F.udf(getFirstAndMiddle, StringType())\n",
    "\n",
    "# Create a new column using your UDF\n",
    "voter_df = voter_df.withColumn('first_and_middle_name', udfFirstAndMiddle(F.array('first_name', 'last_name')))\n",
    "\n",
    "# Show the DataFrame\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an ID Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------+--------+-------------+-------------------+---------+------------------+-----------------------+------------------+--------------------+\n",
      "|      DATE|AGENDA_ITEM_NUMBER|  ITEM_TYPE|DISTRICT|        TITLE|         VOTER NAME|VOTE CAST|FINAL ACTION TAKEN|AGENDA ITEM DESCRIPTION|         AGENDA_ID|             VOTE_ID|\n",
      "+----------+------------------+-----------+--------+-------------+-------------------+---------+------------------+-----------------------+------------------+--------------------+\n",
      "|02/08/2017|                 1|     AGENDA|      13|Councilmember|  Jennifer S. Gates|      N/A|  NO ACTION NEEDED|          Call to Order|020817__Special__1|020817__Special__...|\n",
      "|02/08/2017|                 1|     AGENDA|      14|Councilmember| Philip T. Kingston|      N/A|  NO ACTION NEEDED|          Call to Order|020817__Special__1|020817__Special__...|\n",
      "|02/08/2017|                 1|     AGENDA|      15|        Mayor|Michael S. Rawlings|      N/A|  NO ACTION NEEDED|          Call to Order|020817__Special__1|020817__Special__...|\n",
      "|02/08/2017|                 1|     AGENDA|       2|Councilmember|       Adam Medrano|      N/A|  NO ACTION NEEDED|          Call to Order|020817__Special__1|020817__Special__1_2|\n",
      "|02/08/2017|                 1|     AGENDA|       3|Councilmember|   Casey Thomas, II|      N/A|  NO ACTION NEEDED|          Call to Order|020817__Special__1|020817__Special__1_3|\n",
      "|02/08/2017|                 1|     AGENDA|       4|Councilmember|Carolyn King Arnold|      N/A|  NO ACTION NEEDED|          Call to Order|020817__Special__1|020817__Special__1_4|\n",
      "|02/08/2017|                 4|AGENDA DATE|       1|Councilmember|       Scott Griggs|      YES|          APPROVED|   Authorize a three...|         020817__4|         020817__4_1|\n",
      "|02/08/2017|                 4|AGENDA DATE|      10|Councilmember|   B. Adam  McGough|      YES|          APPROVED|   Authorize a three...|         020817__4|        020817__4_10|\n",
      "|02/08/2017|                 4|AGENDA DATE|      11|Councilmember|       Lee Kleinman|      YES|          APPROVED|   Authorize a three...|         020817__4|        020817__4_11|\n",
      "|02/08/2017|                 4|AGENDA DATE|      12|Councilmember|      Sandy Greyson|      YES|          APPROVED|   Authorize a three...|         020817__4|        020817__4_12|\n",
      "|02/08/2017|                 4|AGENDA DATE|      13|Councilmember|  Jennifer S. Gates|      YES|          APPROVED|   Authorize a three...|         020817__4|        020817__4_13|\n",
      "|02/08/2017|                 4|AGENDA DATE|      14|Councilmember| Philip T. Kingston|      YES|          APPROVED|   Authorize a three...|         020817__4|        020817__4_14|\n",
      "|02/08/2017|                 4|AGENDA DATE|      15|        Mayor|Michael S. Rawlings|      YES|          APPROVED|   Authorize a three...|         020817__4|        020817__4_15|\n",
      "|02/08/2017|                 4|AGENDA DATE|       2|Councilmember|       Adam Medrano|      YES|          APPROVED|   Authorize a three...|         020817__4|         020817__4_2|\n",
      "|02/08/2017|                 4|AGENDA DATE|       3|Councilmember|   Casey Thomas, II|      YES|          APPROVED|   Authorize a three...|         020817__4|         020817__4_3|\n",
      "|02/08/2017|                 4|AGENDA DATE|       4|Councilmember|Carolyn King Arnold|      YES|          APPROVED|   Authorize a three...|         020817__4|         020817__4_4|\n",
      "|02/08/2017|                 4|AGENDA DATE|       5|Councilmember| Rickey D. Callahan|      YES|          APPROVED|   Authorize a three...|         020817__4|         020817__4_5|\n",
      "|01/11/2017|                 1|AGENDA DATE|      13|Councilmember|  Jennifer S. Gates|      YES|          APPROVED|   Approval of Minut...|         011117__1|        011117__1_13|\n",
      "|04/25/2018|                 1|AGENDA DATE|      12|Councilmember|     Sandy  Greyson|      YES|          APPROVED|   Approval of Minut...|         042518__1|        042518__1_12|\n",
      "|04/25/2018|                 1|AGENDA DATE|      13|Councilmember| Jennifer S.  Gates|      YES|          APPROVED|   Approval of Minut...|         042518__1|        042518__1_13|\n",
      "+----------+------------------+-----------+--------+-------------+-------------------+---------+------------------+-----------------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('datasets/DallasCouncilVotes.csv.gz', header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          VOTER NAME|\n",
      "+--------------------+\n",
      "|      Tennell Atkins|\n",
      "|  the  final   20...|\n",
      "|        Scott Griggs|\n",
      "|       Scott  Griggs|\n",
      "|       Sandy Greyson|\n",
      "| Michael S. Rawlings|\n",
      "| the final 2018 A...|\n",
      "|        Kevin Felder|\n",
      "|        Adam Medrano|\n",
      "|                null|\n",
      "|   the   final  2...|\n",
      "|          011018__42|\n",
      "|    Casey Thomas, II|\n",
      "|       Mark  Clayton|\n",
      "|   Casey  Thomas, II|\n",
      "|      Sandy  Greyson|\n",
      "|        Mark Clayton|\n",
      "|  Jennifer S.  Gates|\n",
      "|   Tiffinni A. Young|\n",
      "|  the  final  201...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select all the unique council voters\n",
    "voter_df = df.select(df[\"VOTER NAME\"]).distinct()\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 36 rows in the voter_df DataFrame.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nThere are %d rows in the voter_df DataFrame.\\n\" % voter_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|          VOTER NAME|ROW_ID|\n",
      "+--------------------+------+\n",
      "|      Tennell Atkins|     0|\n",
      "|  the  final   20...|     1|\n",
      "|        Scott Griggs|     2|\n",
      "|       Scott  Griggs|     3|\n",
      "|       Sandy Greyson|     4|\n",
      "| Michael S. Rawlings|     5|\n",
      "| the final 2018 A...|     6|\n",
      "|        Kevin Felder|     7|\n",
      "|        Adam Medrano|     8|\n",
      "|                null|     9|\n",
      "|   the   final  2...|    10|\n",
      "|          011018__42|    11|\n",
      "|    Casey Thomas, II|    12|\n",
      "|       Mark  Clayton|    13|\n",
      "|   Casey  Thomas, II|    14|\n",
      "|      Sandy  Greyson|    15|\n",
      "|        Mark Clayton|    16|\n",
      "|  Jennifer S.  Gates|    17|\n",
      "|   Tiffinni A. Young|    18|\n",
      "|  the  final  201...|    19|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a ROW_ID\n",
    "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|          VOTER NAME|ROW_ID|\n",
      "+--------------------+------+\n",
      "|        Lee Kleinman|    35|\n",
      "|  the  final  201...|    34|\n",
      "|         Erik Wilson|    33|\n",
      "|  the  final   20...|    32|\n",
      "| Carolyn King Arnold|    31|\n",
      "| Rickey D.  Callahan|    30|\n",
      "|   the   final  2...|    29|\n",
      "|    Monica R. Alonzo|    28|\n",
      "|     Lee M. Kleinman|    27|\n",
      "|   Jennifer S. Gates|    26|\n",
      "+--------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the rows with 10 highest IDs in the set\n",
    "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDs with different partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 1 partitions in the voter_df DataFrame.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the number of partitions in each DataFrame\n",
    "print(\"\\nThere are %d partitions in the voter_df DataFrame.\\n\" % voter_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More ID tricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the highest ROW_ID and save it in previous_max_ID\n",
    "previous_max_ID = voter_df_march.select('ROW_ID').rdd.max()[0]\n",
    "\n",
    "# Add a ROW_ID column to voter_df_april starting at the desired value\n",
    "voter_df_april = voter_df_april.withColumn('ROW_ID', F.monotonically_increasing_id() + previous_max_ID)\n",
    "\n",
    "# Show the ROW_ID from both DataFrames and compare\n",
    "voter_df_march.select('ROW_ID').show()\n",
    "voter_df_april.select('ROW_ID').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|       01/01/2017|         0005|                HNL|                          537|\n",
      "|       01/01/2017|         0007|                OGG|                          498|\n",
      "|       01/01/2017|         0037|                SFO|                          241|\n",
      "|       01/01/2017|         0043|                DTW|                          134|\n",
      "|       01/01/2017|         0051|                STL|                           88|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departures_df = spark.read.csv('datasets/AA_DFW_2017_Departures_Short.csv.gz', header=True)\n",
    "departures_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting 139358 rows took 1.007771 seconds\n",
      "Counting 139358 rows again took 0.254597 seconds\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Add caching to the unique rows in departures_df\n",
    "departures_df = departures_df.distinct().cache()\n",
    "\n",
    "# Count the unique rows in departures_df, noting how long the operation takes\n",
    "print(\"Counting %d rows took %f seconds\" % (departures_df.count(), time.time() - start_time))\n",
    "\n",
    "# Count the rows again, noting the variance in time of a cached DataFrame\n",
    "start_time = time.time()\n",
    "print(\"Counting %d rows again took %f seconds\" % (departures_df.count(), time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing a DataFrame from cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is departures_df cached?: True\n",
      "Removing departures_df from cache\n",
      "Is departures_df cached?: False\n"
     ]
    }
   ],
   "source": [
    "# Determine if departures_df is in the cache\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)\n",
    "print(\"Removing departures_df from cache\")\n",
    "\n",
    "# Remove departures_df from the cache\n",
    "departures_df.unpersist()\n",
    "\n",
    "# Check the cache status again\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File import performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the full and split files into DataFrames\n",
    "full_df = spark.read.csv('departures_full.txt.gz')\n",
    "split_df = spark.read.csv('departures_*.txt.gz')\n",
    "\n",
    "# Print the count and run time for each DataFrame\n",
    "start_time_a = time.time()\n",
    "print(\"Total rows in full DataFrame:\\t%d\" % full_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_a))\n",
    "\n",
    "start_time_b = time.time()\n",
    "print(\"Total rows in split DataFrame:\\t%d\" % split_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Spark configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: cleaning-data\n",
      "Driver TCP port: 49835\n",
      "Number of partitions: 200\n"
     ]
    }
   ],
   "source": [
    "app_name = spark.conf.get('spark.app.name')\n",
    "driver_tcp_port = spark.conf.get('spark.driver.port')\n",
    "num_partitions = spark.conf.get('spark.sql.shuffle.partitions')\n",
    "\n",
    "# Show the results\n",
    "print(\"Name: %s\" % app_name)\n",
    "print(\"Driver TCP port: %s\" % driver_tcp_port)\n",
    "print(\"Number of partitions: %s\" % num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dino-mbp'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.driver.host')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Spark configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the number of partitions in variable\n",
    "before = departures_df.rdd.getNumPartitions()\n",
    "# Configure Spark to use 500 partitions\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 500)\n",
    "# Recreate the DataFrame using the departures data file\n",
    "departures_df = spark.read.csv('datasets/AA_DFW_2017_Departures_Short.csv.gz', header=True).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition count before change: 2\n",
      "Partition count after change: 2\n"
     ]
    }
   ],
   "source": [
    "# Print the number of partitions for each instance\n",
    "print(\"Partition count before change: %d\" % before)\n",
    "print(\"Partition count after change: %d\" % departures_df.rdd.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=true\n",
      "+- == Final Plan ==\n",
      "   *(2) HashAggregate(keys=[Date (MM/DD/YYYY)#911, Flight Number#912, Destination Airport#913, Actual elapsed time (Minutes)#914], functions=[])\n",
      "   +- AQEShuffleRead coalesced\n",
      "      +- ShuffleQueryStage 0\n",
      "         +- Exchange hashpartitioning(Date (MM/DD/YYYY)#911, Flight Number#912, Destination Airport#913, Actual elapsed time (Minutes)#914, 500), ENSURE_REQUIREMENTS, [plan_id=959]\n",
      "            +- *(1) HashAggregate(keys=[Date (MM/DD/YYYY)#911, Flight Number#912, Destination Airport#913, Actual elapsed time (Minutes)#914], functions=[])\n",
      "               +- FileScan csv [Date (MM/DD/YYYY)#911,Flight Number#912,Destination Airport#913,Actual elapsed time (Minutes)#914] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/ngohongthai/Documents/projects/IBM-Data-Engineer-Specializ..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Date (MM/DD/YYYY):string,Flight Number:string,Destination Airport:string,Actual elapsed ti...\n",
      "+- == Initial Plan ==\n",
      "   HashAggregate(keys=[Date (MM/DD/YYYY)#911, Flight Number#912, Destination Airport#913, Actual elapsed time (Minutes)#914], functions=[])\n",
      "   +- Exchange hashpartitioning(Date (MM/DD/YYYY)#911, Flight Number#912, Destination Airport#913, Actual elapsed time (Minutes)#914, 500), ENSURE_REQUIREMENTS, [plan_id=949]\n",
      "      +- HashAggregate(keys=[Date (MM/DD/YYYY)#911, Flight Number#912, Destination Airport#913, Actual elapsed time (Minutes)#914], functions=[])\n",
      "         +- FileScan csv [Date (MM/DD/YYYY)#911,Flight Number#912,Destination Airport#913,Actual elapsed time (Minutes)#914] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/ngohongthai/Documents/projects/IBM-Data-Engineer-Specializ..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Date (MM/DD/YYYY):string,Flight Number:string,Destination Airport:string,Actual elapsed ti...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departures_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the flights_df and aiports_df DataFrames\n",
    "normal_df = flights_df.join(airports_df, \\\n",
    "    flights_df[\"Destination Airport\"] == airports_df[\"IATA\"] )\n",
    "\n",
    "# Show the query plan\n",
    "normal_df.explain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using broadcasting on Spark joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "broadcast_df = flights_df.join(broadcast(airports_df), \\\n",
    "    flights_df[\"Destination Airport\"] == airports_df[\"IATA\"] )\n",
    "\n",
    "# Show the query plan and compare against the original\n",
    "broadcast_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing broadcast vs normal joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Count the number of rows in the normal DataFrame\n",
    "normal_count = normal_df.count()\n",
    "normal_duration = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "# Count the number of rows in the broadcast DataFrame\n",
    "broadcast_count = broadcast_df.count()\n",
    "broadcast_duration = time.time() - start_time\n",
    "\n",
    "# Print the counts and the duration of the tests\n",
    "print(\"Normal count:\\t\\t%d\\tduration: %f\" % (normal_count, normal_duration))\n",
    "print(\"Broadcast count:\\t%d\\tduration: %f\" % (broadcast_count, broadcast_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex processing and data pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|       01/01/2015|         0005|                HNL|                          526|\n",
      "|       01/01/2015|         0007|                OGG|                          517|\n",
      "|       01/01/2015|         0023|                SFO|                          233|\n",
      "|       01/01/2015|         0027|                LAS|                          165|\n",
      "|       01/01/2015|         0029|                ONT|                            0|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the data to a DataFrame\n",
    "departures_df = spark.read.csv('datasets/AA_DFW_2015_Departures_Short.csv.gz', header=True)\n",
    "departures_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date (MM/DD/YYYY): string (nullable = true)\n",
      " |-- Flight Number: string (nullable = true)\n",
      " |-- Destination Airport: string (nullable = true)\n",
      " |-- Actual elapsed time (Minutes): string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departures_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146558"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "departures_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143203"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove any duration of 0\n",
    "departures_df = departures_df.filter(departures_df[3] > 0)\n",
    "departures_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+-----------------------------+---+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)| id|\n",
      "+-----------------+-------------+-------------------+-----------------------------+---+\n",
      "|       01/01/2015|         0005|                HNL|                          526|  0|\n",
      "|       01/01/2015|         0007|                OGG|                          517|  1|\n",
      "|       01/01/2015|         0023|                SFO|                          233|  2|\n",
      "|       01/01/2015|         0027|                LAS|                          165|  3|\n",
      "|       01/01/2015|         0035|                HDN|                          178|  4|\n",
      "+-----------------+-------------+-------------------+-----------------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Add an ID column\n",
    "departures_df = departures_df.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "departures_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the file out to JSON format\n",
    "departures_df.write.json('output.json', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing commented lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the file to a DataFrame and perform a row count\n",
    "annotations_df = spark.read.csv('annotations.csv.gz', sep=\"|\")\n",
    "full_count = annotations_df.count()\n",
    "\n",
    "# Count the number of rows beginning with '#'\n",
    "comment_count = annotations_df.where(col('_c0').startswith('#')).count()\n",
    "\n",
    "# Import the file to a new DataFrame, without commented rows\n",
    "no_comments_df = spark.read.csv('annotations.csv.gz', sep='|', comment='#')\n",
    "\n",
    "# Count the new DataFrame and verify the difference is as expected\n",
    "no_comments_count = no_comments_df.count()\n",
    "print(\"Full count: %d\\nComment count: %d\\nRemaining count: %d\" % (full_count, comment_count, no_comments_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing invalid rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split _c0 on the tab character and store the list in a variable\n",
    "tmp_fields = F.split(annotations_df['_c0'], '\\t')\n",
    "\n",
    "# Create the colcount column on the DataFrame\n",
    "annotations_df = annotations_df.withColumn('colcount', F.size(tmp_fields))\n",
    "\n",
    "# Remove any rows containing fewer than 5 fields\n",
    "annotations_df_filtered = annotations_df.filter(~ (annotations_df['colcount'] < 5))\n",
    "\n",
    "# Count the number of rows\n",
    "final_count = annotations_df_filtered.count()\n",
    "print(\"Initial count: %d\\nFinal count: %d\" % (initial_count, final_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the content of _c0 on the tab character (aka, '\\t')\n",
    "split_cols = F.split(annotations_df['_c0'], '\\t')\n",
    "\n",
    "# Add the columns folder, filename, width, and height\n",
    "split_df = annotations_df.withColumn('folder', split_cols.getItem(0))\n",
    "split_df = split_df.withColumn('filename', split_cols.getItem(1))\n",
    "split_df = split_df.withColumn('width', split_cols.getItem(2))\n",
    "split_df = split_df.withColumn('height', split_cols.getItem(3))\n",
    "\n",
    "# Add split_cols as a column\n",
    "split_df = split_df.withColumn('split_cols', split_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever(cols, colcount):\n",
    "  # Return a list of dog data\n",
    "  return cols[4:colcount]\n",
    "\n",
    "# Define the method as a UDF\n",
    "udfRetriever = F.udf(retriever, ArrayType(StringType()))\n",
    "\n",
    "# Create a new column using your UDF\n",
    "split_df = split_df.withColumn('dog_list', udfRetriever(split_df.split_cols, split_df.colcount))\n",
    "\n",
    "# Remove the original column, split_cols, and the colcount\n",
    "split_df = split_df.drop('_c0').drop('split_cols').drop('colcount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate rows via join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column in valid_folders_df\n",
    "valid_folders_df = valid_folders_df.withColumnRenamed(\"_c0\", \"folder\")\n",
    "\n",
    "# Count the number of rows in split_df\n",
    "split_count = split_df.count()\n",
    "\n",
    "# Join the DataFrames\n",
    "joined_df = split_df.join(F.broadcast(valid_folders_df), \"folder\")\n",
    "\n",
    "# Compare the number of rows remaining\n",
    "joined_count = joined_df.count()\n",
    "print(\"Before: %d\\nAfter: %d\" % (split_count, joined_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining invalid rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the row counts for each DataFrame\n",
    "split_count = split_df.count()\n",
    "joined_count = joined_df.count()\n",
    "\n",
    "# Create a DataFrame containing the invalid rows\n",
    "invalid_df = split_df.join(F.broadcast(joined_df), 'folder', 'left_anti')\n",
    "\n",
    "# Validate the count of the new DataFrame is as expected\n",
    "invalid_count = invalid_df.count()\n",
    "print(\" split_df:\\t%d\\n joined_df:\\t%d\\n invalid_df: \\t%d\" % (split_count, joined_count, invalid_count))\n",
    "\n",
    "# Determine the number of distinct folder rows removed\n",
    "invalid_folder_count = invalid_df.select('folder').distinct().count()\n",
    "print(\"%d distinct invalid folders found\" % invalid_folder_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dog parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the dog details and show 10 untruncated rows\n",
    "print(joined_df.select('dog_list').show(10, truncate=False))\n",
    "\n",
    "# Define a schema type for the details in the dog list\n",
    "DogType = StructType([\n",
    "\tStructField(\"breed\", StringType(), False),\n",
    "    StructField(\"start_x\", IntegerType(), False),\n",
    "    StructField(\"start_y\", IntegerType(), False),\n",
    "    StructField(\"end_x\", IntegerType(), False),\n",
    "    StructField(\"end_y\", IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per image count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to return the number and type of dogs as a tuple\n",
    "def dogParse(doglist):\n",
    "  dogs = []\n",
    "  for dog in doglist:\n",
    "    (breed, start_x, start_y, end_x, end_y) = dog.split(',')\n",
    "    dogs.append((breed, int(start_x), int(start_y), int(end_x), int(end_y)))\n",
    "  return dogs\n",
    "\n",
    "# Create a UDF\n",
    "udfDogParse = F.udf(dogParse, ArrayType(DogType))\n",
    "\n",
    "# Use the UDF to list of dogs and drop the old column\n",
    "joined_df = joined_df.withColumn('dogs', udfDogParse('dog_list')).drop('dog_list')\n",
    "\n",
    "# Show the number of dogs in the first 10 rows\n",
    "joined_df.select(F.size('dogs')).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage dog pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to determine the number of pixels per image\n",
    "def dogPixelCount(doglist):\n",
    "  totalpixels = 0\n",
    "  for dog in doglist:\n",
    "    totalpixels += (dog[3] - dog[1]) * (dog[4] - dog[2])\n",
    "  return totalpixels\n",
    "\n",
    "# Define a UDF for the pixel count\n",
    "udfDogPixelCount = F.udf(dogPixelCount, IntegerType())\n",
    "joined_df = joined_df.withColumn('dog_pixels', udfDogPixelCount('dogs'))\n",
    "\n",
    "# Create a column representing the percentage of pixels\n",
    "joined_df = joined_df.withColumn('dog_percent', (joined_df.dog_pixels / (joined_df.width * joined_df.height)) * 100)\n",
    "\n",
    "# Show the first 10 annotations with more than 60% dog\n",
    "joined_df.where('dog_percent > 60').show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
