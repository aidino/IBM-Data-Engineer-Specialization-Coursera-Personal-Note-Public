{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5236599e-b610-45c8-b786-a8ec0991ab2b",
   "metadata": {},
   "source": [
    "# PySpark Basic example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d518796-03e4-4c12-a8e8-b158f87db963",
   "metadata": {},
   "source": [
    "## Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bde62ee9-b2f9-4fde-9663-a620a92adcfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://4c4633122521:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Example 1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4444051850>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark1 = SparkSession.builder.master(\"local[1]\") \\\n",
    ".appName(\"Spark Example 1\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "# Sử dụng local[x]khi chạy ở chế độ Standalone\n",
    "# x phải là một giá trị nguyên và phải lớn hơn 0; \n",
    "# điều này thể hiện số lượng phân vùng cần tạo khi sử dụng RDD, DataFrame và Bộ dữ liệu. \n",
    "# Lý tưởng nhất, giá trị x phải là số lõi CPU bạn có\n",
    "\n",
    "spark1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff3130a4-7d07-49f0-a0fd-b288d2834dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function pyspark.sql.session.SparkSession.newSession(self) -> 'SparkSession'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark2 = SparkSession.newSession\n",
    "spark2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a40394ff-746e-42b3-95fc-576f6bf022c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method SparkSession.Builder.getOrCreate of <pyspark.sql.session.SparkSession.Builder object at 0x7f447ff02110>>\n"
     ]
    }
   ],
   "source": [
    "spark3 = SparkSession.builder.getOrCreate\n",
    "print(spark3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3273041a-b187-4f89-9136-257663d9203e",
   "metadata": {},
   "source": [
    "### Spark Session with Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a300ecfe-131b-461d-be61-6125a1996213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://4c4633122521:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Examples 4</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4444051cd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark1.stop()\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"Spark Examples 4\") \\\n",
    "      .config(\"spark.some.config.option\", \"config-value\") \\\n",
    "      .getOrCreate()\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddfffcf-9d78-417e-a147-951ca966191e",
   "metadata": {},
   "source": [
    "### Create SparkSession with Hive Enable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17a97c9a-3cd3-43d6-b3f4-177f1e33d68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://4c4633122521:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark with Hive</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f447fb1b8d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Enabling Hive to use in Spark\n",
    "spark.stop()\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"Spark with Hive\") \\\n",
    "      .config(\"spark.sql.warehouse.dir\", \"<path>/spark-warehouse\") \\\n",
    "      .enableHiveSupport() \\\n",
    "      .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6f89c1-4ad4-42fd-a9da-bd578555fded",
   "metadata": {},
   "source": [
    "### Create PySpark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17c7bf58-c0c6-4423-94c3-88f65885af73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|   _1|   _2|\n",
      "+-----+-----+\n",
      "|Scala|25000|\n",
      "|Spark|35000|\n",
      "|  PHP|21000|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "df = spark.createDataFrame(\n",
    "    [(\"Scala\", 25000), (\"Spark\", 35000), (\"PHP\", 21000)])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebfdb2e-15da-415d-bb78-da225779228e",
   "metadata": {},
   "source": [
    "### Working with Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e3a73ee-3300-4815-bc5a-0b56c4905ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|   _1|   _2|\n",
      "+-----+-----+\n",
      "|Scala|25000|\n",
      "|Spark|35000|\n",
      "|  PHP|21000|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Spark SQL\n",
    "df.createOrReplaceTempView(\"sample_table\")\n",
    "df2 = spark.sql(\"SELECT _1,_2 FROM sample_table\")\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b1d0e0-c2f5-4e3e-a182-a7a894962322",
   "metadata": {},
   "source": [
    "### Create Hive table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2945df9b-1a8f-402b-a492-1d0be59a3ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|   _1|   _2|\n",
      "+-----+-----+\n",
      "|Scala|25000|\n",
      "|Spark|35000|\n",
      "|  PHP|21000|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create Hive table & query it.  \n",
    "spark.table(\"sample_table\").write.saveAsTable(\"sample_hive_table\")\n",
    "df3 = spark.sql(\"SELECT _1,_2 FROM sample_hive_table\")\n",
    "df3.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa37fbb5-b1e7-4ecd-b7a9-3f05acc06948",
   "metadata": {},
   "source": [
    "###  Working with Catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed443efc-8875-4e5f-a95b-2eb9071ea4bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', catalog='spark_catalog', description='Default Hive database', locationUri='file:/home/jovyan/pyspark-tutorial/%3Cpath%3E/spark-warehouse')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbs = spark.catalog.listDatabases()\n",
    "dbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dd9b570-b36e-4a96-92ae-e2529896bb72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='sample_hive_table', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='sample_table', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbls = spark.catalog.listTables()\n",
    "tbls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a393fb1-7a68-4b2d-9aea-8b611d9d2af5",
   "metadata": {},
   "source": [
    "## PySpark Accumulator (Bộ tích luỹ) with Example\n",
    "\n",
    "`PySpark Accumulator` là một biến dùng chung được sử dụng với RDD và DataFrame để thực hiện các phép tính tổng và bộ đếm tương tự như bộ đếm Map-reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bcdee23-625d-435a-a2ec-8662b1c45f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop previous spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eeec7ffe-c2e4-42ac-bda2-60f82642e9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "-------------\n",
      "15\n",
      "-------------\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"accumulator\").getOrCreate()\n",
    "\n",
    "accum=spark.sparkContext.accumulator(0)\n",
    "rdd=spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "rdd.foreach(lambda x:accum.add(x))\n",
    "print(accum.value)\n",
    "print(\"-------------\")\n",
    "accuSum=spark.sparkContext.accumulator(0)\n",
    "def countFun(x):\n",
    "    global accuSum\n",
    "    accuSum+=x\n",
    "rdd.foreach(countFun)\n",
    "print(accuSum.value)\n",
    "print(\"-------------\")\n",
    "accumCount=spark.sparkContext.accumulator(0)\n",
    "rdd2=spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "rdd2.foreach(lambda x:accumCount.add(1))\n",
    "print(accumCount.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eb73b7-13b8-47c5-9e43-bddbe3a659de",
   "metadata": {},
   "source": [
    "## PySpark `Repartition()` vs `Coalesce()`\n",
    "\n",
    "- `Repartition()`: Phân vùng lại, được sử dụng để tăng hoặc giảm phân vùng RDD/DataFrame\n",
    "- `Coalesce()`: Kết hợp thành một nhóm, được sử dụng để chỉ giảm số lượng phân vùng theo cách hiệu quả.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07dbbfc-b4c1-49b1-8c0d-6a065adaba17",
   "metadata": {},
   "source": [
    "### Trong RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4331fc17-f42e-4914-9630-a363631e7ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From local[1] : 1\n",
      "parallelize : 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create spark session with local[1]\n",
    "rdd = spark.sparkContext.parallelize(range(0,20))\n",
    "print(\"From local[1] : \"+str(rdd.getNumPartitions()))\n",
    "\n",
    "# Use parallelize with 6 partitions\n",
    "rdd1 = spark.sparkContext.parallelize(range(0,25), 2)\n",
    "print(\"parallelize : \"+str(rdd1.getNumPartitions()))\n",
    "\n",
    "# rddFromFile = spark.sparkContext.textFile(\"src/main/resources/test.txt\",10)\n",
    "# print(\"TextFile : \"+str(rddFromFile.getNumPartitions()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81437234-2c86-4612-92da-b4bc4dfab460",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1.saveAsTextFile(\"/tmp/partition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e098bc6-6232-43b3-9de6-88f2fed32be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000  part-00001\t_SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp/partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1e26495-05fc-45f0-b511-87bec9f5d4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "!cat /tmp/partition/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6216b9a-070c-4fe1-a818-ead538efb69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "!cat /tmp/partition/part-00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a12b52e4-5c41-494c-b28c-8eefe7f9922e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartition size : 4\n"
     ]
    }
   ],
   "source": [
    "# Using repartition\n",
    "rdd2 = rdd1.repartition(4)\n",
    "print(\"Repartition size : \"+str(rdd2.getNumPartitions()))\n",
    "rdd2.saveAsTextFile(\"/tmp/re-partition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59ce8b15-8e1e-4a4a-8e26-f83b9ea3bed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000  part-00001\tpart-00002  part-00003\t_SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp/re-partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c416125-4730-4f2d-b53f-02702f78e058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartition size : 3\n",
      "part-00000  part-00001\tpart-00002  _SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Using coalesce()\n",
    "rdd3 = rdd2.coalesce(3)\n",
    "print(\"Repartition size : \"+str(rdd3.getNumPartitions()))\n",
    "rdd3.saveAsTextFile(\"/tmp/coalesce\")\n",
    "!ls /tmp/coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7ec9b40-5255-4601-b7fc-63f0bd956ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "!cat /tmp/coalesce/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45b9c4a1-dc15-4e01-b17a-85b6c15812d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/tmp/hsperfdata_root': Operation not permitted\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "!rm -rf /tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d30f634a-0af0-4ee8-94a5-3e77fe2e8bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hsperfdata_root\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30508a0f-5e10-43c7-b76c-14660f234e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "From local[5]5\n",
      "parallelize : 6\n",
      "Repartition size : 4\n",
      "Repartition size : 4\n"
     ]
    }
   ],
   "source": [
    "# Complete Example\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com') \\\n",
    "        .master(\"local[5]\").getOrCreate()\n",
    "\n",
    "df = spark.range(0,20)\n",
    "print(df.rdd.getNumPartitions())\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"500\")\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(range(0,20))\n",
    "print(\"From local[5]\"+str(rdd.getNumPartitions()))\n",
    "\n",
    "rdd1 = spark.sparkContext.parallelize(range(0,25), 6)\n",
    "print(\"parallelize : \"+str(rdd1.getNumPartitions()))\n",
    "\n",
    "\"\"\"rddFromFile = spark.sparkContext.textFile(\"src/main/resources/test.txt\",10)\n",
    "print(\"TextFile : \"+str(rddFromFile.getNumPartitions())) \"\"\"\n",
    "\n",
    "rdd1.saveAsTextFile(\"/tmp/partition2\")\n",
    "\n",
    "rdd2 = rdd1.repartition(4)\n",
    "print(\"Repartition size : \"+str(rdd2.getNumPartitions()))\n",
    "rdd2.saveAsTextFile(\"/tmp/re-partition2\")\n",
    "\n",
    "rdd3 = rdd1.coalesce(4)\n",
    "print(\"Repartition size : \"+str(rdd3.getNumPartitions()))\n",
    "rdd3.saveAsTextFile(\"/tmp/coalesce2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770711a3-95c0-43ee-a255-24180d49cfab",
   "metadata": {},
   "source": [
    "### Trong Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc1fbf05-60f0-40a1-8c8c-1d5cfbde33d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dde9aa5b-b4a9-4dae-ac49-b439dd20f326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# DataFrame example\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com') \\\n",
    "        .master(\"local[5]\").getOrCreate()\n",
    "\n",
    "df=spark.range(0,20)\n",
    "print(df.rdd.getNumPartitions())\n",
    "\n",
    "df.write.mode(\"overwrite\").csv(\"/tmp/partition\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d7aefc6b-e95e-488e-81a4-c8610a7b308b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-7d5b9a09-31d9-49e0-bdda-42663ef87627-c000.csv\n",
      "part-00001-7d5b9a09-31d9-49e0-bdda-42663ef87627-c000.csv\n",
      "part-00002-7d5b9a09-31d9-49e0-bdda-42663ef87627-c000.csv\n",
      "part-00003-7d5b9a09-31d9-49e0-bdda-42663ef87627-c000.csv\n",
      "part-00004-7d5b9a09-31d9-49e0-bdda-42663ef87627-c000.csv\n",
      "_SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp/partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39c2f077-3998-482c-910a-fecc3fe5d14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "!cat /tmp/partition/part-00004-7d5b9a09-31d9-49e0-bdda-42663ef87627-c000.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "35c37db1-ae99-45f9-a512-d727d7b0e3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "13\n",
      "14\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "!cat /tmp/partition/part-00003-7d5b9a09-31d9-49e0-bdda-42663ef87627-c000.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a4ef75-b48c-4c27-b715-1b8b909bc022",
   "metadata": {},
   "source": [
    "## PySpark Broadcast Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb08867-88ee-4e97-85ac-ca1ac02a4609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
